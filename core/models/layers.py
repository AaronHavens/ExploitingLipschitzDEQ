import logging
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

def cpad(x, ksize):
  p = ksize // 2
  r = (ksize % 2 == 0)*1
  return F.pad(x, (p, p-r, p, p-r), 'circular')

class SDPBasedLipschitzConvLayer(nn.Module):

  def __init__(self, config, cin, cout, kernel_size=3, epsilon=1e-6):
    super(SDPBasedLipschitzConvLayer, self).__init__()

    self.activation = nn.ReLU(inplace=False)

    kernel = torch.randn(cout, cin, kernel_size, kernel_size)
    bias = torch.randn(cout)
    q = torch.rand(cout)
    self.kernel = nn.Parameter(kernel)
    self.bias = nn.Parameter(bias)
    self.q = nn.Parameter(q)

    self.epsilon = epsilon

  def forward(self, x):
    x_size = x.shape[-1]
    x_pad = cpad(x, self.kernel.shape[-1])
    res = F.conv2d(x_pad, self.kernel, bias=self.bias, padding=0)
    res = self.activation(res)
    batch_size, cout, x_size, x_size = res.shape
    kkt = F.conv2d(self.kernel, self.kernel, padding=self.kernel.shape[-1] - 1)
    q_abs = torch.abs(self.q)
    T = 2 / (torch.abs(q_abs[None, :, None, None] * kkt).sum((1, 2, 3)) / q_abs)
    res = T[None, :, None, None] * res
    res_pad = cpad(res, self.kernel.shape[-1])
    res = F.conv2d(res_pad, self.kernel.permute(1, 0, 2, 3).flip(2, 3), padding=0)
    out = x - res
    return out  


class SDPBasedLipschitzLinearLayer(nn.Module):

  def __init__(self, config, cin, cout, epsilon=1e-6):
    super(SDPBasedLipschitzLinearLayer, self).__init__()

    weights = torch.randn(cout, cin)
    bias = torch.randn(cout)
    q = torch.rand(cout)
    self.weights = nn.Parameter(weights)
    self.bias = nn.Parameter(bias)
    self.q = nn.Parameter(q)

    self.activation = nn.ReLU(inplace=False)
    self.epsilon = epsilon

  def forward(self, x):
    res = F.linear(x, self.weights, self.bias)
    res = self.activation(res)
    q_abs = torch.abs(self.q)
    q = q_abs[None, :]
    q_inv = (1/q_abs)[:, None]
    T = 2/torch.abs(q_inv * self.weights @ self.weights.T * q).sum(1)
    res = T * res
    res = F.linear(res, self.weights.t())
    out = x - res
    return out




class PaddingChannels(nn.Module):

  def __init__(self, ncout, ncin=3):
    super(PaddingChannels, self).__init__()
    self.ncout = ncout
    self.ncin = ncin

  def forward(self, x):
    bs, _, size1, size2 = x.shape
    out = torch.zeros(bs, self.ncout, size1, size2, device=x.device)
    out[:, :self.ncin] = x
    return out


class PoolingLinear(nn.Module):

  def __init__(self, ncin, ncout):
    super(PoolingLinear, self).__init__()
    self.ncout = ncout
    self.ncin = ncin

  def forward(self, x):
    return x[:, :self.ncout]


