import time
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.linalg import sqrtm

testing = lambda x, y: np.testing.assert_array_almost_equal(
  x.numpy(), y.numpy(), decimal=1)

def is_sdp(M):
  eigvals = torch.linalg.eigvals(M) + 1e-5
  return all(eigvals.real.detach().numpy() > 0)

def safe_inv(v):
  v = torch.nan_to_num(v, posinf=0)
  mask = (v != 0)
  v_inv = torch.zeros_like(v)
  v_inv[mask] = 1 / v[mask]
  return v_inv

def extend_weights(weights):
  cout, cin = weights.shape
  weights_ = torch.zeros(cout, cout)
  weights_[:, :cin] = weights
  return weights_

def init_diag_t_dense(w, q):
  q_abs = torch.abs(q)[None, :]
  q_abs_inv = safe_inv(torch.abs(q))[:, None]
  T = 2/torch.abs(q_abs_inv * w @ w.T * q_abs).sum(1)
  Tinv = safe_inv(T)
  return T, Tinv


class DenseLipschitzDEQ(nn.Module):

  def __init__(self, config, cin, cout):
    super(DenseLipschitzDEQ, self).__init__()

    self.make_checks = config.mon_checks
    self.ckpt_path = config.ckpt_path
    self.cin, self.cout = cin, cout
    self.depth = config.dense_depth

    self.eps = 1e-8
    self.lip = 1

    self.init_model_sll()

    # init parameters
    bias = torch.cat(self.bias_weights)
    self.M = nn.Parameter(self.M.cpu().to(torch.float64))
    self.V = nn.Parameter(self.V.cpu().to(torch.float64))
    self.U = nn.Parameter(self.U.cpu().to(torch.float64))
    self.Lambda = nn.Parameter(self.Lambda.cpu().to(torch.float64))
    self.G = nn.Parameter(self.G.cpu().to(torch.float64))
    self.bias_weights = nn.Parameter(bias.cpu().to(torch.float64))

  @torch.no_grad()
  def init_model_sll(self):
    
    depth = self.depth
    cin, cout = self.cin, self.cout

    self.Wlist, self.q = [], []
    self.bias_weights = []
    self.bias_weights.append(torch.zeros(cout))
    
    ckpt = torch.load(self.ckpt_path, map_location='cpu')['model_state_dict']
    w_names = list(filter(lambda x: 'denses' in x and 'weights' in x, ckpt))
    bias_names = list(filter(lambda x: 'denses' in x and 'bias' in x, ckpt))
    q_names = list(filter(lambda x: 'denses' in x and 'q' in x, ckpt))

    for i in range(len(w_names)):
      self.Wlist.append(extend_weights(ckpt[w_names[i]]).to(torch.float64))
      self.q.append(ckpt[q_names[i]].to(torch.float64))
      self.bias_weights.append(ckpt[bias_names[i]].to(torch.float64))

    self.eye = torch.eye((depth+1)*cout).to(torch.float64)
    self.eye1 = torch.eye(cout).to(torch.float64)
    self.Z = torch.zeros(cout, cout).to(torch.float64)

    # init weights
    self.Glist, self.Tlist, self.Tinvlist = [], [], []
    for i in range(depth):
      W, q = self.Wlist[i], self.q[i]
      # init T matrices
      T, Tinv = init_diag_t_dense(W, q)
      self.Tlist.append(T)
      self.Tinvlist.append(Tinv)
      # init G matrice
      G = -W.T * T.reshape(1, -1)
      self.Glist.append(G)

    # init products
    self.WGlist = []
    for i in range(depth-1):
      for j in range(i+1, depth):
        WG = self.Wlist[j] @ self.Glist[i]
        self.WGlist.append(WG)

    # init W
    Fulllist = self.Wlist + self.WGlist
    k = 0
    Wfull = torch.zeros((depth+1, cout, depth+1, cout))
    for i in range(0, depth+1):
      for j in range(i+1, depth+1):
        Wfull[j, :, i, :] = Fulllist[k]
        k += 1
    Wfull = Wfull.reshape((depth+1)*cout, (depth+1)*cout)
    
    # init Psi and Lambda
    ones = torch.ones(cout).to(torch.float64)
    self.Psi = torch.cat([ones] + self.Tinvlist).to(torch.float64)
    self.Lambda = torch.cat([ones] + self.Tlist).to(torch.float64)
    self.Psiinv = self.Lambda
    
    # init M, S anf V
    self.eye_zeros = self.eye1.clone()
    self.eye_zeros[:, cin:] = 0
    self.G = torch.cat([self.eye_zeros] + self.Glist, dim=1)
    self.U = torch.cat([self.eye_zeros] + depth*[self.Z], dim=0)
    
    self.eye_ = self.eye * (self.Psi * self.Psiinv)

    GtG = self.G.T @ self.G
    UUt = self.U @ self.U.T
    LUUtL = self.Lambda.reshape(-1, 1) * UUt * self.Lambda.reshape(-1, 1)
    M = self.Lambda.reshape(-1, 1) * (self.eye_ - Wfull) - (1/(2*self.lip) * (GtG + LUUtL))
    self.M = M

    V = torch.from_numpy(sqrtm((1/2 * (M + M.T)).numpy()).real.astype(np.float64))
    self.V = V
    
    if self.make_checks:
      testing(V @ V, 1/2 * (M + M.T))
      print('VtV, ok')
      is_sdp(1/2 * (M + M.T))
      print('sdp ok')
      Wfull2 = self.get_w_init()
      print('max diff', torch.abs(Wfull - Wfull2).max())
      testing(Wfull, Wfull2)
      print('check correctness of decomposition of Wfull, ok')

  def x_shape(self, n_batch):
    return (n_batch, self.cin)

  def z_shape(self, n_batch):
    return ((n_batch, (self.depth+1)*self.cout), )

  def forward(self, x, *z):
    bias = self.bias_weights
    x = F.pad(x, (0, abs(self.cin-self.cout)))
    x = x.to(self.U.dtype)
    out = self.multiply(*z)[0] + F.linear(x, self.U, bias)
    return (out, )

  def bias(self, x):
    bias = self.bias_weights
    x = F.pad(x, (0, abs(self.cin-self.cout)))
    x = x.to(self.U.dtype)
    out = F.linear(x, self.U, bias)
    return (out, )

  def forward_g(self, *z):
    out = F.linear(z[0], self.G)
    return out

  def get_w_init(self):
    self.eye = self.eye.to(self.M.device).to(self.M.dtype)
    self.eye_ = self.eye_.to(self.M.device).to(self.M.dtype)
    S = 1/2 * (self.M - self.M.T)
    VtV = self.V @ self.V
    UUt = self.U @ self.U.T
    L = torch.abs(self.Lambda)
    LUUtL = L.reshape(-1, 1) * UUt * L.reshape(1, -1)
    GtG = self.G.T @ self.G
    inner = 1/(2*self.lip) * GtG + 1/(2*self.lip) * LUUtL + VtV + S
    Psi = safe_inv(L)
    W = self.eye_ - (Psi.reshape(-1, 1) * inner)
    return W

  def get_w(self):
    self.eye = self.eye.to(self.M.device).to(self.M.dtype)
    self.eye_ = self.eye_.to(self.M.device).to(self.M.dtype)
    S = 1/2 * (self.M - self.M.T)
    VtV = self.V @ self.V
    UUt = self.U @ self.U.T
    L = torch.abs(self.Lambda)
    LUUtL = L.reshape(-1, 1) * UUt * L.reshape(1, -1)
    GtG = self.G.T @ self.G
    inner = 1/(2*self.lip) * GtG + 1/(2*self.lip) * LUUtL + VtV + self.eps * self.eye + S
    Psi = safe_inv(L)
    W = self.eye_ - (Psi.reshape(-1, 1) * inner)
    return W

  def multiply(self, *z):
    w = self.get_w()
    z_out = F.linear(z[0], w)
    return (z_out, )
  
  def multiply_transpose(self, *g):
    w = self.get_w()
    g_out = F.linear(g[0], w.T)
    return (g_out, )
  
  def init_inverse(self, alpha, beta):
    w = self.get_w()
    self.w_inv = torch.linalg.solve(
      alpha * self.eye + beta * w, self.eye)
  
  def inverse(self, *z):
    return (F.linear(z[0], self.w_inv), )
  
  def inverse_transpose(self, *g):
    return (F.linear(g[0], self.w_inv.T), ) 

  
