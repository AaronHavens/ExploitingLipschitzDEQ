import time
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.fft import rfftn, irfftn, fftn, ifftn, fftshift, ifftshift
from scipy.linalg import sqrtm

testing = lambda x, y: np.testing.assert_array_almost_equal(
  x.cpu().numpy(), y.cpu().numpy(), decimal=1)


def is_kernel_sdp(kernel, x_size):
  kfft = kernel_fft(kernel, x_size)
  c1, c2, ksize, _ = kfft.shape
  kfft = kfft.permute(2, 3, 0, 1).reshape(-1, c1, c2)
  eigvals = np.array([np.linalg.eigvals(x) for x in kfft.cpu().numpy()]).reshape(-1) + 1e-5
  assert all(eigvals > 0), f'min eigenvals: {min(eigvals)}'

def init_kernel(cout, cin, ksize):
  weight = torch.empty(cout, cin, ksize, ksize)
  bias = torch.empty(cout)
  n = cin
  for k in (ksize, ksize):
      n *= k
  stdv = 1. / np.sqrt(n)
  weight.data.normal_(0, 0.01)
  bias.data.uniform_(-stdv, stdv)
  if cout < cin:
    p = cin-cout
    padding = (0, 0, 0, 0, 0, 0, 0, p)
    weight = F.pad(weight, padding)
  return weight, bias


def safe_inv(v):
  v = torch.nan_to_num(v, posinf=0)
  mask = (v != 0)
  v_inv = torch.zeros_like(v)
  v_inv[mask] = 1 / v[mask]
  return v_inv

def init_diag_t(kernel, q, cout, cin):
  q = torch.cat((q, torch.zeros(cin-cout).to(kernel.device)))
  p = kernel.shape[-1] - 1
  kkt = F.conv2d(kernel, kernel, padding=p)
  q_abs = torch.abs(q)
  T = (2 * q_abs) * safe_inv(torch.abs(q_abs[None, :, None, None] * kkt).sum((1, 2, 3)))
  Tinv = safe_inv(T)
  return T[None, :, None, None], Tinv[None, :, None, None]

def transpose_k(kernel, x_size):
  kernel = kernel.permute(1, 0, 2, 3).flip(2, 3)
  if kernel.shape[-1] == x_size and kernel.shape[-1] % 2 == 0:
    kernel = kernel.roll((-1, -1), (2, 3))
  return kernel

def cpad(x, ksize):
  p = ksize // 2
  r = (ksize % 2 == 0)*1
  return F.pad(x, (p, p-r, p, p-r), 'circular')

def pad_to_xsize(kernel, x_size):
  if kernel.shape[-1] == x_size: return kernel
  p = (x_size - kernel.shape[-1]) // 2 
  r = (x_size % 2 == 0)*1
  padding = (p, p+r, p, p+r)
  k_pad = F.pad(kernel, padding)
  assert k_pad.shape[-1] == x_size, f'k_pad: {k_pad.shape}'
  return k_pad

def product_conv(k1, k2, x_size):
  origin_device = k1.device
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  k1, k2 = k1.to(device), k2.to(device)
  k1_pad = pad_to_xsize(k1, x_size)
  k1k2 = F.conv2d(cpad(k1_pad, k2.shape[-1]), k2.permute(1, 0, 2, 3).flip(2, 3), padding=0)
  assert k1k2.shape[-1] == x_size, f'k1k2: {k1k2.shape}'
  return k1k2.to(origin_device)

def init_keye(cout, cin, ksize, x_size):
  keye = torch.zeros(cout, cin, ksize, ksize)
  p = (x_size - ksize) // 2
  r = (x_size % 2 == 0)*1
  nn.init.dirac_(keye)
  padding = (p, p+r, p, p+r)
  keye = F.pad(keye, padding)
  if cout < cin:
    p = cin - cout
    padding = (0, 0, 0, 0, 0, 0, 0, p)
    keye = F.pad(keye, padding)
  return keye

def kernel_fft(kernel, x_size):
  origin_device = kernel.device
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  kernel = kernel.to(device)
  k_shift = ifftshift(pad_to_xsize(kernel, x_size))
  if x_size % 2 == 0:
      k_shift = k_shift.roll((1, 1), (2, 3))
  return fftn(k_shift, s=(x_size, x_size), dim=(2, 3)).to(origin_device)

def kernel_ifft(kfft, x_size):
  origin_device = kfft.device
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  kfft = kfft.to(device)
  kernel = ifftn(kfft, (x_size, x_size), (2, 3))
  if x_size % 2 == 0:
    kernel = kernel.roll((-1, -1), (2, 3))
  return fftshift(kernel).to(origin_device)

def kernel_sqrt(kernel, x_size):
  origin_device = kernel.device
  kfft = kernel_fft(kernel, x_size)
  c1, c2, ksize, _ = kfft.shape
  assert ksize == x_size
  kfft = kfft.permute(2, 3, 0, 1).reshape(-1, c1, c2)
  kfft_sqrt = torch.from_numpy(np.array([sqrtm(x) for x in kfft.cpu().numpy()])).to(torch.complex64)
  kfft_sqrt = kfft_sqrt.permute(1, 2, 0).reshape(c1, c2, x_size, x_size)
  return kernel_ifft(kfft_sqrt, x_size).to(origin_device)

def transpose_kfft(kfft):
  kfft = kfft.permute(1, 0, 2, 3).conj()
  return kfft
  
def multi_channels_conv_fft(a, b):
  """Multiplies two complex-valued tensors."""
  a = a.view(a.size(0), 1, -1, *a.shape[2:])
  b = b.view(1, -1, *b.shape[1:])
  a = torch.movedim(a, 2, a.dim() - 1).unsqueeze(-2)
  b = torch.movedim(b, (1, 2), (b.dim() - 1, b.dim() - 2))
  c = a @ b
  c = torch.movedim(c, c.dim() - 1, 2).squeeze(-1)
  return c.view(c.size(0), -1, *c.shape[3:])

def fft_conv(x, kfft, x_size):
  origin_device = x.device
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  x = x.to(device)
  kfft = kfft.to(device)
  r = (x_size % 2 == 0)*1
  x_fft = fftn(ifftshift(x), (x_size, x_size), (2, 3))
  output_fft = multi_channels_conv_fft(x_fft, kfft.conj())
  output = fftshift(ifftn(output_fft, (x_size, x_size), (2, 3)))
  return output.real.to(origin_device)

def kernel_inv_fft(k_fft, x_size):
  origin_device = k_fft.device
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  k_fft = k_fft.to(device)
  c1, c2, ksize, _ = k_fft.shape
  assert ksize == x_size
  k_fft = k_fft.permute(2, 3, 0, 1).reshape(-1, c1, c2)
  batch_eye = torch.eye(c1).repeat(x_size**2, 1).reshape(
    -1, c1, c2).to(k_fft.dtype).to(k_fft.device)
  k_fft_inv = torch.linalg.solve(k_fft, batch_eye)
  k_fft_inv = k_fft_inv.permute(1, 2, 0).reshape(c1, c2, x_size, x_size)
  return k_fft_inv.to(origin_device)

def extend_kernel(kernel):
  cout, cin = kernel.shape[:2]
  if cout < cin:
    p = cin-cout
    padding = (0, 0, 0, 0, 0, 0, 0, p)
    kernel = F.pad(kernel, padding)
  return kernel

def extend_bias(bias, cout, cin):
  device = bias.device
  bias = torch.cat((bias, torch.zeros(cin-cout).to(device)))
  return bias


class ConvLipschitzDEQ(nn.Module):

  def __init__(self, config, x_size, cin, inner_dim, kernel_size=3):
    super(ConvLipschitzDEQ, self).__init__()

    self.make_checks = config.mon_checks
    self.ckpt_path = config.ckpt_path
    self.depth = config.conv_depth

    self.x_size = x_size
    self.cin, self.cout = cin, inner_dim
    self.ksize = ksize = kernel_size

    self.eps = 1e-8
    self.lip = 1

    self.init_model_sll()

    bias = torch.cat(self.bias_weights)[None, :, None, None]

    self.get_w = self._get_w_from_decomp

    # init parameters
    self.kM = nn.Parameter(self.kM.cpu())
    self.kV = nn.Parameter(self.kV.cpu())
    self.kU = nn.Parameter(self.kU.cpu())
    self.kLambda = nn.Parameter(self.kLambda.cpu())
    self.kG = nn.Parameter(self.kG.cpu())
    self.bias_weights = nn.Parameter(bias.cpu())

  @torch.no_grad()
  def init_model_sll(self):
    
    depth = self.depth
    x_size = self.x_size
    cin, cout = self.cin, self.cout
    ksize = self.ksize
   
    self.kWlist, self.q = [], []
    self.bias_weights = []
    self.bias_weights.append(torch.zeros(cin))

    ckpt = torch.load(self.ckpt_path, map_location='cpu')['model_state_dict']
    k_names = list(filter(lambda x: 'convs' in x and 'kernel' in x, ckpt))
    bias_names = list(filter(lambda x: 'convs' in x and 'bias' in x, ckpt))
    q_names = list(filter(lambda x: 'convs' in x and 'q' in x, ckpt))

    for i in range(len(k_names)):
      W = extend_kernel(ckpt[k_names[i]])
      self.kWlist.append(W)
      bias = extend_bias(ckpt[bias_names[i]], cout, cin)
      self.bias_weights.append(bias)
      q = ckpt[q_names[i]]
      self.q.append(q)

    self.keye = init_keye((depth+1)*cin, (depth+1)*cin, ksize, x_size)
    self.keye1 = init_keye(cin, cin, ksize, x_size)
    self.kZ = torch.zeros(cin, cin, x_size, x_size)

    # init weights
    self.kGlist, self.kTlist, self.kTinvlist = [], [], []
    for i in range(depth):
      kW, q = self.kWlist[i], self.q[i]
      # init T matrices
      kT, kTinv = init_diag_t(kW, q, cout, cin)
      self.kTlist.append(kT)
      self.kTinvlist.append(kTinv)
      # init G matrice
      kG = pad_to_xsize(-transpose_k(kW, x_size) * kT, x_size)
      self.kGlist.append(kG)

    # init products
    self.kWGlist = []
    for i in range(depth-1):
      for j in range(i+1, depth):
        kWG = product_conv(self.kWlist[j], self.kGlist[i], x_size)
        self.kWGlist.append(kWG)

    # init W
    kWlist_pad = [pad_to_xsize(kW, x_size) for kW in self.kWlist]
    kFulllist = kWlist_pad + self.kWGlist
    k = 0
    kWfull = torch.zeros((depth+1, cin, depth+1, cin, x_size, x_size))
    for i in range(0, depth+1):
      for j in range(i+1, depth+1):
        kWfull[j, :, i, :] = kFulllist[k]
        k += 1
    kWfull = kWfull.reshape((depth+1)*cin, (depth+1)*cin, x_size, x_size)
    self.mask = (kWfull == 0)*1.
    self.kWfull = kWfull

    # init Psi and Lambda
    ones = torch.ones(1, cin, 1, 1)
    self.kPsi = torch.cat([ones] + self.kTinvlist, dim=1)
    self.kLambda = torch.cat([ones] + self.kTlist, dim=1)
    self.kPsiinv = self.kLambda
    
    # init M, S anf V 
    self.kG = torch.cat([self.keye1] + self.kGlist, dim=1)
    self.kU = torch.cat([self.keye1] + depth*[self.kZ], dim=0)
    
    self.keye_ = self.keye * (self.kPsi * self.kPsiinv)

    kGtG = product_conv(transpose_k(self.kG, x_size), self.kG, x_size)
    kUUt = product_conv(self.kU, transpose_k(self.kU, x_size), x_size)
    kLambdaUUt = self.kLambda.permute(1, 0, 2, 3) * kUUt
    kLambdaUUtLambda = kLambdaUUt * self.kLambda
    kM = self.kLambda.permute(1, 0, 2, 3) * (self.keye_ - kWfull) - \
        (1/(2*self.lip) * (kGtG + kLambdaUUtLambda))
    self.kM = kM

    kV = kernel_sqrt(1/2 * (kM + transpose_k(kM, x_size)), x_size).real
    self.kV = kV

    if self.make_checks:
      # check PSD of VtV -> kV should be real
      is_kernel_sdp(1/2 * (self.kM + transpose_k(self.kM, x_size)), x_size)
      print('sdp ok')
      kWfull2 = self._get_w_init()
      testing(kWfull, kWfull2)
      print('check correctness of decomposition of Wfull, ok')

  def x_shape(self, n_batch):
    return (n_batch, self.cin, self.x_size, self.x_size)

  def z_shape(self, n_batch):
    return ((n_batch, (self.depth+1)*self.cin, self.x_size, self.x_size), )

  def forward(self, x, *z):
    U_fft = kernel_fft(self.kU, self.x_size)
    bias = self.bias_weights
    x = x.to(self.kU.dtype)
    out = self.multiply(*z)[0] + fft_conv(x, U_fft, self.x_size) + bias
    return (out, )

  def bias(self, x):
    U_fft = kernel_fft(self.kU, self.x_size)
    bias = self.bias_weights
    x = x.to(self.kU.dtype)
    out_fft = fft_conv(x, U_fft, self.x_size)
    out = fft_conv(x, U_fft, self.x_size) + bias
    return (out, )

  def forward_g(self, *z):
    G_fft = kernel_fft(self.kG, self.x_size)
    out = fft_conv(z[0], G_fft, self.x_size)
    return out

  def _get_wfull(self, do_fft=False):
    if do_fft:
      return kernel_fft(self.kWfull, self.x_size)
    return self.kWfull

  def _get_w_from_decomp(self, do_fft=False):
    self.keye = self.keye.to(self.kM.device)
    self.keye_ = self.keye_.to(self.kM.device)
    kS = 1/2 * (self.kM - transpose_k(self.kM, self.x_size))
    kVtV = product_conv(self.kV, transpose_k(self.kV, self.x_size), self.x_size)
    kUUt = product_conv(self.kU, transpose_k(self.kU, self.x_size), self.x_size)
    kLambda = torch.abs(self.kLambda)
    kLambdaUUt = kLambda.permute(1, 0, 2, 3) * kUUt
    kLambdaUUtLambda = kLambdaUUt * kLambda
    kGtG = product_conv(transpose_k(self.kG, self.x_size), self.kG, self.x_size)
    kinner = 1/(2*self.lip) * kGtG + 1/(2*self.lip) * kLambdaUUtLambda + kVtV + self.eps * self.keye + kS
    kPsi = safe_inv(kLambda)
    kW = self.keye_ - (kPsi.permute(1, 0, 2, 3) * kinner)
    if do_fft:
      return kernel_fft(kW, self.x_size)
    return kW

  def _get_w_init(self):
    self.keye = self.keye.to(self.kM.device)
    self.keye_ = self.keye_.to(self.kM.device)
    kS = 1/2 * (self.kM - transpose_k(self.kM, self.x_size))
    kVtV = product_conv(self.kV, transpose_k(self.kV, self.x_size), self.x_size)
    kUUt = product_conv(self.kU, transpose_k(self.kU, self.x_size), self.x_size)
    kLambda = torch.abs(self.kLambda)
    kLambdaUUt = kLambda.permute(1, 0, 2, 3) * kUUt
    kLambdaUUtLambda = kLambdaUUt * kLambda
    kGtG = product_conv(transpose_k(self.kG, self.x_size), self.kG, self.x_size)
    kinner = 1/(2*self.lip) * kGtG + 1/(2*self.lip) * kLambdaUUtLambda + kVtV + kS
    kPsi = safe_inv(kLambda)
    kW = self.keye_ - (kPsi.permute(1, 0, 2, 3) * kinner)
    return kW

  def kernel_regularization(self, p=2):
    kW = self.get_w()
    self.mask = self.mask.to(kW.device)
    return (kW * self.mask).reshape(-1).norm(p)

  def get_wfft(self):
    return self.get_w(do_fft=True)

  def multiply(self, *z):
    wfft = self.get_wfft()
    z_out = fft_conv(z[0], wfft, self.x_size)
    return (z_out, )
  
  def multiply_transpose(self, *g):
    wfft = self.get_wfft()
    g_out = fft_conv(g[0], transpose_kfft(wfft), self.x_size)
    return (g_out, )
  
  def init_inverse(self, alpha, beta):
    wfft = self.get_wfft()
    keye_fft = kernel_fft(self.keye, self.x_size).to(wfft.device)
    torch.cuda.empty_cache()
    self.wfft_inv = kernel_inv_fft(alpha * keye_fft + beta * wfft, self.x_size)
  
  def inverse(self, *z):
    return (fft_conv(z[0], self.wfft_inv, self.x_size), )
  
  def inverse_transpose(self, *g):
    return (fft_conv(g[0], transpose_kfft(self.wfft_inv), self.x_size), ) 





