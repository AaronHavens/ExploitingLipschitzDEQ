import os
import sys
import time
import random
import datetime
import pprint
import socket
import logging
import glob
from os.path import join, exists
from contextlib import nullcontext

from core import utils
from core.models.model import NormalizedModel
from core.models.model import LipschitzNetworkDEQ, SLLNetwork
from core.data.readers import readers_config

import numpy as np
import torch
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch import nn
from torch.nn.parallel import DistributedDataParallel
import torch.distributed as dist
from torch.distributed.elastic.multiprocessing.errors import record


class Trainer:
  """A Trainer to train a PyTorch."""

  def __init__(self, config):
    self.config = config

  def _save_ckpt(self, step, epoch, final=False, best=False):
    """Save ckpt in train directory."""
    freq_ckpt_epochs = self.config.save_checkpoint_epochs
    if (epoch % freq_ckpt_epochs == 0 and self.is_master \
        and epoch not in self.saved_ckpts) \
         or (final and self.is_master) or best:
      prefix = "model" if not best else "best_model"
      ckpt_name = f"{prefix}.ckpt-{step}.pth"
      os.makedirs(join(self.train_dir, 'checkpoints'), exist_ok=True)
      ckpt_path = join(self.train_dir, 'checkpoints', ckpt_name)
      if exists(ckpt_path) and not best: return 
      self.saved_ckpts.add(epoch)
      state = {
        'epoch': epoch,
        'global_step': step,
        'model_state_dict': self.model.state_dict(),
        'optimizer_state_dict': self.optimizer.state_dict(),
      }
      logging.debug("Saving checkpoint '{}'.".format(ckpt_name))
      torch.save(state, ckpt_path)

  @torch.no_grad()
  def test_eval_at_init(self):
    logging.info('-- Accuracy at initialization --')
    self.model.eval()
    running_accuracy = torch.zeros(1).cuda()
    running_certified = torch.zeros(4).cuda()
    running_inputs = torch.zeros(1).cuda()
    data_loader, _ = self.reader.load_dataset()
    eps_list = np.array([36, 72, 108, 255])
    eps_float_list = eps_list / 255
    Reader = readers_config[self.config.dataset]
    reader_test = Reader(self.config, self.batch_size, self.is_distributed, is_training=False)
    data_loader, _ = reader_test.load_dataset()
    for batch_n, data in enumerate(data_loader):
      inputs, labels = data
      inputs, labels = inputs.cuda(), labels.cuda()
      outputs = self.model(inputs)
      predicted = outputs.argmax(axis=1)
      correct = outputs.max(1)[1] == labels
      margins = torch.sort(outputs, 1)[0]
      for i, eps_float in enumerate(eps_float_list):
        certified = (margins[:, -1] - margins[:, -2]) > np.sqrt(2.) * eps_float
        running_certified[i] += torch.sum(correct & certified).item()
      running_accuracy += predicted.eq(labels.data).sum()
      running_inputs += inputs.size(0)
    if self.is_distributed:
      torch.distributed.all_reduce(running_accuracy)
      torch.distributed.all_reduce(running_certified)
      torch.distributed.all_reduce(running_inputs)
    accuracy = running_accuracy / running_inputs
    certified = running_certified / running_inputs
    if self.local_rank == 0:
      self.message.add('accuracy', accuracy.item(), format='.5f')
      self.message.add('certified accuracy', list(certified.cpu().numpy()), format='.5f')
      logging.info(self.message.get_message())
    self.model.train()

  @record
  def __call__(self):
    """Performs training and evaluation
    """
    cudnn.benchmark = True

    self.train_dir = self.config.train_dir

    if os.environ.get('RANK') is not None:
      self.rank = int(os.environ['RANK'])
      self.local_rank = int(os.environ['LOCAL_RANK'])
      self.num_nodes = int(os.environ['LOCAL_WORLD_SIZE']) 
      self.num_tasks = int(os.environ['WORLD_SIZE'])
      self.is_master = bool(self.rank == 0)
      self.ngpus = 1
    else:
      self.rank, self.local_rank = 0, 0
      self.num_nodes, self.num_tasks = 1, 1
      self.ngpus = torch.cuda.device_count()

    # Setup logging
    utils.setup_logging(self.config, self.rank)

    self.message = utils.MessageBuilder()
    # print self.config parameters
    if self.local_rank == 0:
      logging.info(self.config.cmd)
      pp = pprint.PrettyPrinter(indent=2, compact=True)
      logging.info(pp.pformat(vars(self.config)))
    # print infos
    if self.local_rank == 0:
      logging.info(f"PyTorch version: {torch.__version__}.")
      logging.info(f"NCCL Version {torch.cuda.nccl.version()}")
      logging.info(f"Hostname: {socket.gethostname()}.")

    # ditributed settings
    self.world_size = 1
    self.is_distributed = False
    if self.num_nodes > 1 or self.num_tasks > 1:
      self.is_distributed = True
      self.world_size = self.num_nodes * self.ngpus
    if self.num_nodes > 1:
      logging.info(
        f"Distributed Training on {self.num_nodes} nodes")
    elif self.num_nodes == 1 and self.num_tasks > 1:
      logging.info(f"Single node Distributed Training with {self.num_tasks} tasks")
    else:
      assert self.num_nodes == 1 and self.num_tasks == 1
      logging.info("Single node training.")

    if not self.is_distributed:
      self.batch_size = self.config.batch_size * self.ngpus
    else:
      self.batch_size = self.config.batch_size

    self.global_batch_size = self.batch_size * self.world_size
    logging.info('World Size={} => Total batch size {}'.format(
      self.world_size, self.global_batch_size))

    if self.is_distributed:
      torch.cuda.set_device(self.local_rank)

    # load dataset
    Reader = readers_config[self.config.dataset]
    self.reader = Reader(self.config, self.batch_size, self.is_distributed, is_training=True)
    if self.local_rank == 0:
      logging.info(f"Using dataset: {self.config.dataset}")

    if self.config.mode == 'train-sll':
      self.model = SLLNetwork(self.config, self.reader.n_classes)
    elif self.config.mode == 'finetune-deq':
      self.model = LipschitzNetworkDEQ(self.config, self.reader.n_classes)
    self.model = NormalizedModel(self.model, self.reader.means, self.reader.stds)
    self.model = self.model.cuda()

    # setup distributed process if training is distributed 
    # and use DistributedDataParallel for distributed training
    if self.is_distributed:
      utils.setup_distributed_training(self.world_size, self.rank)
      self.model = DistributedDataParallel(
        self.model, device_ids=[self.local_rank], output_device=self.local_rank)
      if self.local_rank == 0:
        logging.info('Model defined with DistributedDataParallel')
    else:
      self.model = nn.DataParallel(self.model, device_ids=range(torch.cuda.device_count()))

    nb_parameters = np.sum([p.numel() for p in self.model.parameters() if p.requires_grad])
    logging.info(f'Number of parameters to train: {nb_parameters}')

    if self.config.mode == 'finetune-deq':
      # check test eval of model
      self.test_eval_at_init()
      logging.info('Done with eval')

    # define set for saved ckpt
    self.saved_ckpts = set([0])

    # define optimizer
    betas = (0.5, 0.9)
    self.optimizer = torch.optim.Adam(
      self.model.parameters(), lr=self.config.lr, weight_decay=0., betas=betas)

    # define learning rate scheduler
    num_steps = self.config.epochs * (self.reader.n_train_files // self.global_batch_size)
    self.scheduler = utils.TriangularLRScheduler(self.optimizer, num_steps, self.config.lr)

    # define the loss
    self.criterion = utils.LossXent(self.config)

    data_loader, sampler = self.reader.load_dataset()
    if sampler is not None:
      assert sampler.num_replicas == self.world_size

    if self.is_distributed:
      n_files = sampler.num_samples
    else:
      n_files = self.reader.n_train_files

    start_epoch, global_step = 0, 0

    if self.local_rank == 0:
      logging.info("Number of files on worker: {}".format(n_files))
      logging.info("Start training")

    # training loop
    for epoch_id in range(start_epoch, self.config.epochs):
      if self.is_distributed:
        sampler.set_epoch(epoch_id)
      for n_batch, data in enumerate(data_loader):
        if global_step == 2 and self.is_master:
          start_time = time.time()
        epoch = (int(global_step) * self.global_batch_size) / self.reader.n_train_files
        self.one_step_training(data, epoch, global_step)
        self._save_ckpt(global_step, epoch_id)
        if global_step == 20 and self.is_master:
          self._print_approximated_train_time(start_time)
        global_step += 1

    self._save_ckpt(global_step, epoch_id, final=True)
    logging.info("Done training -- epoch limit reached.")

  def _print_approximated_train_time(self, start_time):
    total_steps = self.reader.n_train_files * self.config.epochs / self.global_batch_size
    total_seconds = total_steps * ((time.time() - start_time) / 18)
    n_days = total_seconds // 86400
    n_hours = (total_seconds % 86400) / 3600
    logging.info(
      'Approximated training time: {:.0f} days and {:.1f} hours'.format(
        n_days, n_hours))

  def _to_print(self, step):
    frequency = self.config.frequency_log_steps
    if frequency is None:
      return False
    return (step % frequency == 0 and self.local_rank == 0) or \
        (step == 1 and self.local_rank == 0)

  def process_gradients(self, step):
    if self.config.gradient_clip_by_norm:
      if step == 0 and self.local_rank == 0:
        logging.info("Clipping Gradient by norm: {}".format(
          self.config.gradient_clip_by_norm))
      torch.nn.utils.clip_grad_norm_(
        self.model.parameters(), self.config.gradient_clip_by_norm)

  def one_step_training(self, data, epoch, step):

    self.optimizer.zero_grad()

    batch_start_time = time.time()
    images, labels = data
    images, labels = images.cuda(), labels.cuda()

    if step == 0 and self.local_rank == 0:
      logging.info(f'images {images.shape}')
      logging.info(f'labels {labels.shape}')

    outputs = self.model(images)
    if step == 0 and self.local_rank == 0:
      logging.info(f'outputs {outputs.shape}')

    loss = self.criterion(outputs, labels)
    if self.config.kernel_reg > 0:
      p = self.config.kernel_reg_norm
      kernel_reg_fn = self.model.module.model.mon_conv.linear_module
      reg = self.config.kernel_reg * kernel_reg_fn.kernel_regularization(p)
      total_loss = loss + reg
      total_loss.backward()
    else:
      loss.backward()
    self.process_gradients(step)
    self.optimizer.step()
    self.scheduler.step(step)

    seconds_per_batch = time.time() - batch_start_time
    examples_per_second = self.batch_size / seconds_per_batch
    examples_per_second *= self.world_size

    if self._to_print(step):
      lr = self.optimizer.param_groups[0]['lr']
      self.message.add("epoch", epoch, format="4.2f")
      self.message.add("step", step, width=5, format=".0f")
      self.message.add("lr", lr, format=".10f")
      if self.config.kernel_reg > 0:
        self.message.add("total_loss", total_loss, format=".4f")
        self.message.add("loss", loss, format=".4f")
        self.message.add("reg", reg, format=".4f")
      else:
        self.message.add("loss", loss, format=".4f")
      self.message.add("imgs/sec", examples_per_second, width=5, format=".0f")
      logging.info(self.message.get_message())



