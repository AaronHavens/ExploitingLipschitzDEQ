import json
import time
import os
import re
import glob
import socket
import pprint
import logging
import shutil
from os.path import join, exists, basename

from core import utils
from core.models.model import NormalizedModel
from core.models.model import LipschitzNetworkDEQ, SLLNetwork
from core.data.readers import readers_config

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch.nn.parallel import DistributedDataParallel


class Evaluator:
  """Evaluate a Pytorch Model."""

  def __init__(self, config):
    self.config = config

  def load_ckpt(self, ckpt_path=None):
    checkpoints = glob.glob(join(self.config.train_dir, "checkpoints", "model.ckpt-*.pth"))
    get_model_id = lambda x: int(x.strip('.pth').strip('model.ckpt-'))
    ckpt_name = sorted([ckpt.split('/')[-1] for ckpt in checkpoints], key=get_model_id)[-1]
    ckpt_path = join(self.config.train_dir, "checkpoints", ckpt_name)
    checkpoint = torch.load(ckpt_path)
    self.model.load_state_dict(checkpoint['model_state_dict'], strict=True)

  def __call__(self):
    """Run evaluation of model or eval under attack"""

    cudnn.benchmark = True

    if os.environ.get('RANK') is not None:
      self.rank = int(os.environ['RANK'])
      self.local_rank = int(os.environ['LOCAL_RANK'])
      self.num_nodes = int(os.environ['LOCAL_WORLD_SIZE']) 
      self.num_tasks = int(os.environ['WORLD_SIZE'])
      self.is_master = bool(self.rank == 0)
      self.ngpus = 1
    else:
      self.rank, self.local_rank = 0, 0
      self.num_nodes, self.num_tasks = 1, 1
      self.ngpus = torch.cuda.device_count()

    # create a mesage builder for logging
    self.message = utils.MessageBuilder()
    # Setup logging & log the version.
    utils.setup_logging(self.config, self.rank)

    # ditributed settings
    self.world_size = 1
    self.is_distributed = False
    if self.num_nodes > 1 or self.num_tasks > 1:
      self.is_distributed = True
      self.world_size = self.num_nodes * self.ngpus
    if self.num_nodes > 1:
      logging.info(
        f"Distributed Evaluation on {self.num_nodes} nodes")
    elif self.num_nodes == 1 and self.num_tasks > 1:
      logging.info(f"Single node Distributed Evaluation with {self.num_tasks} tasks")
    else:
      assert self.num_nodes == 1 and self.num_tasks == 1
      logging.info("Single node Evaluation.")

    if not self.is_distributed:
      self.batch_size = self.config.batch_size * self.ngpus
    else:
      self.batch_size = self.config.batch_size

    self.global_batch_size = self.batch_size * self.world_size
    if self.local_rank == 0:
      logging.info('World Size={} => Total batch size {}'.format(
        self.world_size, self.global_batch_size))

    if self.is_distributed:
      torch.cuda.set_device(self.local_rank)

    # load dataset
    Reader = readers_config[self.config.dataset]
    self.reader = Reader(self.config, self.batch_size, self.is_distributed, is_training=False)
    if self.local_rank == 0:
      logging.info(f"Using dataset: {self.config.dataset}")

    # load model
    if self.config.mode == 'eval-sll':
      self.model = SLLNetwork(self.config, self.reader.n_classes)
    elif self.config.mode == 'eval-deq':
      self.model = LipschitzNetworkDEQ(self.config, self.reader.n_classes)
    self.model = NormalizedModel(self.model, self.reader.means, self.reader.stds)
    self.model = self.model.cuda()

    # setup distributed process if training is distributed 
    # and use DistributedDataParallel for distributed training
    if self.is_distributed:
      utils.setup_distributed_training(self.world_size, self.rank)
      self.model = DistributedDataParallel(
        self.model, device_ids=[self.local_rank], output_device=self.local_rank)
      if self.local_rank == 0:
        logging.info('Model defined with DistributedDataParallel')
    else:
      self.model = nn.DataParallel(self.model, device_ids=range(self.ngpus))

    self.load_ckpt()
    self.model.eval()
    self.eval()

    logging.info("Done with batched inference.")


  @torch.no_grad()
  def eval(self):
    running_accuracy = torch.zeros(1).cuda()
    running_certified = torch.zeros(4).cuda()
    running_inputs = torch.zeros(1).cuda()
    data_loader, _ = self.reader.load_dataset()
    eps_list = np.array([36, 72, 108, 255])
    eps_float_list = eps_list / 255
    data_loader, _ = self.reader.load_dataset()
    for batch_n, data in enumerate(data_loader):
      inputs, labels = data
      inputs, labels = inputs.cuda(), labels.cuda()
      outputs = self.model(inputs)
      predicted = outputs.argmax(axis=1)
      correct = outputs.max(1)[1] == labels
      margins = torch.sort(outputs, 1)[0]
      for i, eps_float in enumerate(eps_float_list):
        certified = (margins[:, -1] - margins[:, -2]) > np.sqrt(2.) * eps_float
        running_certified[i] += torch.sum(correct & certified).item()
      running_accuracy += predicted.eq(labels.data).sum()
      running_inputs += inputs.size(0)

    if self.is_distributed:
      torch.distributed.all_reduce(running_accuracy)
      torch.distributed.all_reduce(running_certified)
      torch.distributed.all_reduce(running_inputs)

    accuracy = running_accuracy / running_inputs
    certified = running_certified / running_inputs
    if self.local_rank == 0:
      self.message.add('accuracy', accuracy.item(), format='.5f')
      self.message.add('certified accuracy', list(certified.cpu().numpy()), format='.5f')
      logging.info(self.message.get_message())


